<h1>Project: Data Warehouse</h1>


<h2>Introduction</h2>
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

We have to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to.




<h2>Project Datasets</h2>

<ol>
<li>Song data: s3://udacity-dend/song_data</li>
<li>Log data: s3://udacity-dend/log_data</li>
<li>This third file s3://udacity-dend/log_json_path.jsoncontains the meta information that is required by AWS to correctly load s3://udacity-dend/log_data</li>
</ol>


<h3>Song Dataset</h3>
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
Below is an example of what a single song file json, looks like.

<pre><code>{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
</code></pre>

<h3>Log Dataset</h3>
The second dataset consists of log files in JSON format generated by a event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
The log files in the dataset  are partitioned by year and month

<h2>Schema for Song Play Analysis</h2>
Using the song and event datasets, I have created a star schema optimized for queries on song play analysis. This includes the following tables.

<h3>Fact Table</h3>
___songplays___ - records in event data associated with song plays i.e. records with page NextSong<br>
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

<h3>Dimension Tables</h3>
___users___- users in the app<br>
user_id, first_name, last_name, gender, level<br>

___songs___ - songs in music database<br>
song_id, title, artist_id, year, duration

___artists___ - artists in music database<br>
artist_id, name, location, latitude, longitude

___time___ - timestamps of records in songplays broken down into specific units<br>
start_time, hour, day, week, month, year, weekday



<h2>Project Steps</h2>
Below are the project steps.

<h3>Create Table Schemas</h3>
<ol>
<li>Design schemas for your fact and dimension tables</li>
<li>Write a SQL CREATE statement for each of these tables in sql_queries.py</li>
<li>Complete the logic in create_tables.py to connect to the database and create these tables</li>
<li>Write SQL DROP statements to drop tables in the beginning of create_tables.py if the tables already exist. This way, you can run create_tables.py whenever you want to reset your database and test your ETL pipeline.</li>
<li>Launch a redshift cluster and create an IAM role that has read access to S3.</li>
<li>Add redshift database and IAM role info to dwh.cfg.</li>
Test by running create_tables.py and checking the table schemas in your redshift database. You can use Query Editor in the AWS Redshift console for this.</li>
</ol>
<h3>Build ETL Pipeline</h3>
<ol>
<li>Implement the logic in etl.py to load data from S3 to staging tables on Redshift.</li>
<li>Implement the logic in etl.py to load data from staging tables to analytics tables on Redshift.</li>
</ol>


<h2>Table Design Strategy</h2>

<ol>
<li>Songplays Table: I created a sort key on start_time , since time column with sorted values are much more efficient. <br>
I also analysed the number of records in other dimension tables and since songs table had the ,most number of records , it would be prudent to distribute the data based on song_id. I checked the count of song_id present in songplays and estimated that the distribution cannot be skewed by ,much since many songs occur with count 1-10 with exception of 1 song. This is the reason I did not use dist style even</li>
<li>
Users Table: Since there are only 104 records in here , I used Dist style ALL here.
</li>
<li>
songs Table : I used dist style even here , since there are relatively large records in here . Also One possible optimisation that can be used here is dist key of artist_id , which will group similar artist song together.
</li>
<li>
times Table : Used start_time as sort key for similar reasons as songplays.
</li>
</ol>


<h2>Analytic Queries Examples </h2>

I have added some sample Bussiness questions that can be answered with this dataset , in seperate sql_example_queries.py.
More questions can be added here.
